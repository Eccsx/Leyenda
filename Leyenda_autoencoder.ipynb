{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8387b4",
   "metadata": {},
   "source": [
    "<p style='text-align: center'>\n",
    "    <img src='images/cesi.png' width=\"20%\">\n",
    "    <div style='text-align: center'>Rima Benrejeb, Thomas Mattone, Bastien Reynaud, Badreddine Ferragh</div>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leyenda - Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b32f8b",
   "metadata": {
    "papermill": {
     "duration": 0.011808,
     "end_time": "2022-09-05T05:05:38.744112",
     "exception": false,
     "start_time": "2022-09-05T05:05:38.732304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Table Of Contents**\n",
    "> 1. [Objective](#1)\n",
    "> 2. [Data](#2)\n",
    "> 3. [Notebook imports](#3)\n",
    "> 4. [Data preparation](#4)\n",
    "> 5. [Hyper-parameters](#5)\n",
    "> 6. [Deep Neural Networks](#6)\n",
    "> 7. [Convolutional Neural Networks](#7)\n",
    "> 11. [Results](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7ff07",
   "metadata": {},
   "source": [
    "## Objective <a class=\"anchor\" id=\"1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss the denoising of photos using algorithms relied on convolution auto-encoders in order to facilitate their processing.\n",
    "\n",
    "Please note that I you run this notebook code on your own, **you may encounter different accuracy results** than the ones we obtained. <br>\n",
    "This is due on the way the `model.evaluate()` function works in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56063a0",
   "metadata": {},
   "source": [
    "## Data <a class=\"anchor\" id=\"2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 150 .jpg images, comming in different dimensions.\n",
    "\n",
    "Because of the low number of images, in order to obtain significants results, we will augment this dataset to increase the number of trainable items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25efed4",
   "metadata": {
    "papermill": {
     "duration": 0.009516,
     "end_time": "2022-09-05T05:05:38.764094",
     "exception": false,
     "start_time": "2022-09-05T05:05:38.754578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook imports <a class=\"anchor\" id=\"3\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# File manipulation\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Input, Flatten, ReLU, Reshape\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Image manipulation\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug as ia\n",
    "import imghdr\n",
    "from PIL import ImageFile\n",
    "\n",
    "# Options for PIL\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "ia.seed(42)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import visualkeras\n",
    "\n",
    "# Options for seaborn\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Utils\n",
    "import leyenda_utils as lu\n",
    "%watermark -p watermark,wget,numpy,sklearn,tensorflow,PIL,matplotlib,seaborn,visualkeras,imgaug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5a4aa",
   "metadata": {
    "papermill": {
     "duration": 0.009795,
     "end_time": "2022-09-05T05:05:50.884801",
     "exception": false,
     "start_time": "2022-09-05T05:05:50.875006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data preparation <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we verify the validaity of the images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e85fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-04T10:10:37.632746Z",
     "iopub.status.busy": "2022-10-04T10:10:37.632588Z",
     "iopub.status.idle": "2022-10-04T10:10:37.636800Z",
     "shell.execute_reply": "2022-10-04T10:10:37.636404Z",
     "shell.execute_reply.started": "2022-10-04T10:10:37.632729Z"
    },
    "papermill": {
     "duration": 0.024834,
     "end_time": "2022-09-05T05:05:50.963379",
     "exception": false,
     "start_time": "2022-09-05T05:05:50.938545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/image_noise'\n",
    "\n",
    "data_dir = pathlib.Path(os.path.join(DATA_PATH))\n",
    "invalid_images = []\n",
    "\n",
    "for file in list(data_dir.glob('*/*.*')):\n",
    "    if imghdr.what(file) not in ['jpeg', 'png']:\n",
    "        invalid_images.append(file)\n",
    "        \n",
    "print(f'{len(invalid_images)} invalids images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55269192",
   "metadata": {},
   "source": [
    "In the case we encounter no-usable images, we move them into a different directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from skimage import io\n",
    "\n",
    "images = []\n",
    "\n",
    "for path in glob.glob(DATA_PATH + '/*'):\n",
    "    images.append(io.imread(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the images, and preprocess them into a `IMG_H` by `IMG_W` size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "IMG_H, IMG_W = 180, 180\n",
    "\n",
    "preprocess = iaa.Sequential([\n",
    "    iaa.Resize((IMG_H, IMG_W))\n",
    "])\n",
    "\n",
    "images_pre = np.array(preprocess(images=images))\n",
    "\n",
    "print(images_pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To augment our image base, we apply the following transformation on each basic images:\n",
    "- an horizontal and vertical flip, each with 50% chance\n",
    "- a zoom-in between 1 and 1.5\n",
    "- a gamma and sigmoid contrast modification\n",
    "\n",
    "This procedure is repeated as many time as the `AUGMENTED_FACTOR` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTATION_FACTOR = 10\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.HorizontalFlip(0.5),\n",
    "    iaa.VerticalFlip(0.5),\n",
    "    iaa.Affine(scale=(1, 1.5)),\n",
    "    iaa.GammaContrast((0.5, 1),\n",
    "                      per_channel=True),\n",
    "    iaa.SigmoidContrast(gain=(3, 10),\n",
    "                        cutoff=(0.4, 0.6),\n",
    "                        per_channel=True)\n",
    "])\n",
    "\n",
    "images_aug = images_pre\n",
    "\n",
    "for i in range(AUGMENTATION_FACTOR):\n",
    "    images_aug = np.concatenate((images_aug, seq(images=images_pre)), axis=0)\n",
    "    \n",
    "print(images_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to split the augmented dataset into training, validation and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = np.array([0.8, 0.1, 0.1])\n",
    "\n",
    "train_split, val_split, test_split = np.array_split(images_aug,\n",
    "                                                    (fractions[:-1].cumsum() * len(images_aug)).astype(int))\n",
    "                                                    \n",
    "print(train_split.shape)\n",
    "print(val_split.shape)\n",
    "print(test_split.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a noised version of each set by randomly adding some gaussian and laplace noise.\n",
    "\n",
    "In the same time, we perform feature scaling by mapping the images pixel values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = iaa.Sequential([\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.2 * 255),\n",
    "                              per_channel=True),\n",
    "    iaa.AdditiveLaplaceNoise(scale=(0, 0.2 * 255))\n",
    "])\n",
    "\n",
    "train = train_split / 255\n",
    "val = val_split / 255\n",
    "test = test_split / 255\n",
    "\n",
    "train_noise = noise(images=train_split).astype(np.float32) / 255\n",
    "val_noise = noise(images=val_split).astype(np.float32) / 255\n",
    "test_noise = noise(images=test_split).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu.compare_image_sets([train, train_noise], 10,\n",
    "                      labels=['original', 'noise'],\n",
    "                      size=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97a656",
   "metadata": {},
   "source": [
    "## Hyper-parameters <a class=\"anchor\" id=\"5\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to compare the model troughout the notebook, we chose to training them using the same configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCH = 1000\n",
    "LOSS = MeanSquaredError()\n",
    "OPTIMIZER = Adam(1e-3)\n",
    "METRICS = [lu.SSIM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure to accruacy during, we will use the `SSIM` metric.\n",
    "\n",
    "SSIM is a value allowing the compare the similarity between two images:\n",
    "- **1** indicates perfect similarity\n",
    "- **0** indicates no similarity\n",
    "- **-1** indicates perfect anti-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder <a class=\"anchor\" id=\"6\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This autoencoder architecture works on the basis of 2 concepts that will allow us to take a noisy image as input:\n",
    "- **Encoder**: The purpose of this step is to downsample the input image. To do this, we will use convolution operations as seen previously in the first notebook. This will result in reducing the size of the input image. As a result of the convolution operations, we will obtain a vector representation of our image, called **latent space**.\n",
    "- **Decoder**: The second part, called decoder, takes as input the latent space generated by the encoder. Its objective will be to reconstruct the image by removing the noise. To do this, we use layers of transposed convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoder.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0318b",
   "metadata": {
    "papermill": {
     "duration": 0.01395,
     "end_time": "2022-09-05T05:05:55.944564",
     "exception": false,
     "start_time": "2022-09-05T05:05:55.930614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Deep Neural Network <a class=\"anchor\" id=\"6\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05a8c2",
   "metadata": {},
   "source": [
    "We began with a dummy `dnn_1`, to be able to visualize how behave a poorly designed network.\n",
    "\n",
    "It is composed of `Dense` layers activated with ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0afe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_1 = Sequential([\n",
    "    Flatten(input_shape=(IMG_H, IMG_W, 3)),\n",
    "    ####\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    ####\n",
    "    Dense(units=32, activation='relu'),\n",
    "    ####\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=IMG_H * IMG_W * 3, activation='relu'),\n",
    "    ####\n",
    "    Reshape(target_shape=(IMG_H, IMG_W, 3))\n",
    "], name='dnn_1')\n",
    "\n",
    "dnn_1.compile(loss=LOSS,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "visualkeras.layered_view(dnn_1, scale_xy=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d591e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lu.is_model_already_trained(dnn_1):\n",
    "    lu.load_model_training(dnn_1)\n",
    "else:\n",
    "    dnn_1.fit(train_noise, train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs= NUM_EPOCH,\n",
    "              validation_data = (val_noise, val))\n",
    "            \n",
    "    lu.save_model_training(dnn_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the `loss` nad `val_loss` going down quickly, the `accuracy` struggle to pass the 35% and the model is overfitting a bit with a `val_accuracy` over 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu.plot_model_history(dnn_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dnn_1` barely reach the 30% of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_1.evaluate(test_noise, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a lokk at the reconstructed image, we can see nevertheless that the model produce 'ghost' images with the global shape and colors of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dnn_1 = dnn_1.predict(test_noise)\n",
    "\n",
    "lu.compare_image_sets([test, test_noise, preds_dnn_1], 10,\n",
    "                      labels=['orignal', 'noise', 'dnn_1'],\n",
    "                      size=(15, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51a237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-03T13:56:26.895649Z",
     "iopub.status.busy": "2022-09-03T13:56:26.895264Z",
     "iopub.status.idle": "2022-09-03T13:56:26.900645Z",
     "shell.execute_reply": "2022-09-03T13:56:26.899279Z",
     "shell.execute_reply.started": "2022-09-03T13:56:26.895618Z"
    },
    "papermill": {
     "duration": 0.013261,
     "end_time": "2022-09-05T05:05:55.971949",
     "exception": false,
     "start_time": "2022-09-05T05:05:55.958688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolutional Neural Network <a class=\"anchor\" id=\"7\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already explained in the previous notebook, the reason of the bad performance of `dnn_1` is that Deep Neural Networks are not designed for image analysis.\n",
    "\n",
    "Convolutional Neural Networks helps detecting image features such as edges, color gradient or other parameters making an image unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4d05d",
   "metadata": {},
   "source": [
    "<img src='images/convolution.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwing model includes 4 `Conv2D` layers activated by a ReLU function and a latent space of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1 = Sequential([\n",
    "    Input(shape=(IMG_H, IMG_W, 3)), \n",
    "    ###\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    ###\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    ###\n",
    "    Conv2D(filters=3, kernel_size=(3, 3), padding='same', activation='sigmoid')\n",
    "], name='cnn_1')\n",
    "\n",
    "cnn_1.compile(loss=LOSS,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "visualkeras.layered_view(cnn_1, scale_xy=0.9, scale_z=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c080fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lu.is_model_already_trained(cnn_1):\n",
    "    lu.load_model_training(cnn_1)\n",
    "else:\n",
    "    cnn_1.fit(train_noise, train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCH,\n",
    "              validation_data = (val_noise, val))\n",
    "            \n",
    "    lu.save_model_training(cnn_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adn we can already see a big improvement compare to `dnn_1`.\n",
    "\n",
    "`cnn_1` follow the same behavior as its predecessor but the `loss` and `val_loss` decrease below 0.003. <br>\n",
    "On the other side, the `accuracy` and `val_accuracy` are much closer and above the 75% mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu.plot_model_history(cnn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu.load_model_training(dnn_1)\n",
    "\n",
    "lu.plot_models_history([dnn_1, cnn_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the testing set, `cnn_1` reaches 78% of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.evaluate(test_noise, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reconstructions are much closer to the originals. <br>\n",
    "However, we can still notice some image artefacts due to the amount of noise that was applied at the beggining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cnn_1 = cnn_1.predict(test_noise)\n",
    "\n",
    "lu.compare_image_sets([test, test_noise, preds_cnn_1], 10,\n",
    "                      labels=['orignal', 'noise', 'cnn_1'],\n",
    "                      size=(15, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074e6b7",
   "metadata": {},
   "source": [
    "## Results <a class=\"anchor\" id=\"8\"></a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu.compare_image_sets([test, test_noise, preds_dnn_1, preds_cnn_1], 10,\n",
    "                      labels=['original', 'noise', 'dnn_1', 'cnn_1'],\n",
    "                      size=(15, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('leyenda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "427540e104d00e12c644bde433a22f99519267736e4c29c31afd89a435d79c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
